\chapter{Πειραματική Αξιολόγηση και Αποτελέσματα}
\label{ch:experiments}

Στο κεφάλαιο αυτό παρουσιάζεται η πειραματική διαδικασία που ακολουθήθηκε
για την εκπαίδευση και συγκριτική αξιολόγηση διαφορετικών αρχιτεκτονικών
τμηματοποίησης σε \en{3D MRI}, καθώς και τα αντίστοιχα αποτελέσματα.

Η επιλογή των μοντέλων προέκυψε μέσα από συστηματική μελέτη της βιβλιογραφίας:
αρχικά εξετάστηκαν τα θεμελιώδη \en{U-shaped} δίκτυα (\en{U-Net})
\cite{ronneberger2015unet} και στη συνέχεια παραλλαγές/επεκτάσεις όπως το
\en{Attention U-Net} \cite{oktay2018attentionunet} και σύγχρονες \en{U-Net} μορφές
όπως το \en{DynUNet} \cite{isensee2021nnunet}.
Τέλος, αξιολογήθηκαν νεότερες προσεγγίσεις που μοντελοποιούν μακρινές
εξαρτήσεις, όπως τα \en{Transformer-based} μοντέλα
(\en{UNETR} \cite{hatamizadeh2022unetr},
\en{SwinUNETR} \cite{hatamizadeh2022swinunetr})
και \en{State Space-based} μοντέλα
(\en{SegMamba} \cite{xing2024segmamba}, βασισμένο στο \en{Mamba}
\cite{gu2023mamba}).

Κεντρικός στόχος είναι η \textbf{δίκαιη και αναπαραγώγιμη σύγκριση} των
αρχιτεκτονικών. Για τον λόγο αυτό υλοποιήθηκε ένα \textbf{ενιαίο πειραματικό
πλαίσιο} όπου όλες οι αρχιτεκτονικές εκπαιδεύονται με κοινή ροή
(\en{pipeline}) προεπεξεργασίας, δειγματοληψίας \en{patches}, εκπαίδευσης,
αξιολόγησης και καταγραφής μετρικών.
Στην πράξη, κάθε αρχιτεκτονική αντιστοιχεί σε ξεχωριστό \en{notebook},
όμως η δομή του κώδικα παραμένει σταθερή, με κύρια διαφοροποίηση την
αρχικοποίηση του μοντέλου και ελάχιστες αναγκαίες ρυθμίσεις συμβατότητας.

\section{Υλοποίηση και λογισμικό}
\label{sec:exp_software}

Η υλοποίηση πραγματοποιήθηκε στο \en{PyTorch},
με χρήση του \en{MONAI} \cite{cardoso2022monai}, το οποίο παρέχει έτοιμες
αρχιτεκτονικές, μετασχηματισμούς (\en{transforms}), συναρτήσεις απώλειας,
και εργαλεία αξιολόγησης κατάλληλα για \en{3D} ιατρική τμηματοποίηση.

Όλες οι αρχιτεκτονικές (εκτός του \en{SegMamba}, που ενσωματώθηκε ως
εξωτερική υλοποίηση) χρησιμοποιούν κοινές συνιστώσες του \en{MONAI} για:
(α) ανάγνωση \en{NIfTI} δεδομένων, (β) προεπεξεργασία/επαύξηση,
(γ) \en{sliding-window inference}, και (δ) υπολογισμό μετρικών.

\section{Σύνολο δεδομένων και διαχωρισμός}
\label{sec:exp_data}

Το σύνολο δεδομένων περιλαμβάνει $N=137$ περιστατικά, με ένα ζεύγος αρχείων
\en{\texttt{.nii.gz}} ανά περίπτωση: έναν ογκομετρικό όγκο \en{MRI} και την
αντίστοιχη δυαδική μάσκα τμηματοποίησης πλακούντα.
Ως βασική μονάδα εκπαίδευσης και αξιολόγησης ορίζεται το \en{case}
(δηλαδή ολόκληρος ο \en{3D} όγκος), ώστε ο διαχωρισμός να γίνεται σε επίπεδο
ασθενή/περίπτωσης και να αποφεύγεται διαρροή πληροφορίας.

Ο διαχωρισμός σε σύνολα εκπαίδευσης και επικύρωσης πραγματοποιείται με
σταθερό \en{seed} ($121$) για λόγους αναπαραγωγιμότητας.
Στα πειράματα της παρούσας εργασίας χρησιμοποιήθηκε διαχωρισμός $80/20$,
που αντιστοιχεί περίπου σε $109$ περιστατικά εκπαίδευσης και $28$ επικύρωσης.

\section{Προεπεξεργασία}
\label{sec:exp_preproc}

Οι μετασχηματισμοί για την προεπεξεργασία επιλέχθηκαν απο την βιβλιοθήκη της 
\en{MONAI} \cite{monaitransforms} ώστε να διακρίνονται πιο πολύ οι περιοχές ενδιαφέροντος,
να διευκολύνουμε το μοντέλο να γενικεύεται πιο ιδανικά και να 
ελαττωθεί το υπολογιστικό κόστος σε ένα ιδιαίτερα αραιό \en{foreground} πρόβλημα. 

Το βασικό \en{pipeline} προεπεξεργασίας (κοινό για όλα τα μοντέλα) περιλαμβάνει:
\begin{itemize}
  \item \textbf{Επαναπροσανατολισμό} σε κοινό σύστημα αξόνων (\en{RAS}).
  \item \textbf{Αναδειγματοληψία} σε στοχευμένο \en{voxel spacing}
        $(2.0,2.0,2.0)$\,\en{mm}, για συγκρισιμότητα μεταξύ περιστατικών.
  \item \textbf{Κανονικοποίηση εντάσεων} με \en{percentiles} (2--99.9) και
        χαρτογράφηση σε $[0,1]$.
  \item \textbf{\en{Foreground cropping}} με \en{CropForegroundd} και
        \en{source\_key=label}, με περιθώριο $m=8$ \en{voxels}, ώστε να
        απομονώνεται περιοχή ενδιαφέροντος (περιορισμός \en{FOV}) και να
        μειώνεται η σπατάλη σε \en{background}.
  \item \textbf{\en{Padding}} σε σταθερό \en{roi\_size} $(96,96,64)$ και
        επιπλέον \textbf{\en{divisible padding}} (σε $(32,32,16)$) ώστε οι
        τελικές διαστάσεις να είναι συμβατές με τις απαιτήσεις των
        αρχιτεκτονικών.
\end{itemize}

Για επιτάχυνση της εκπαίδευσης, οι μετασχηματισμοί του βασικού \en{pipeline}
προϋπολογίζονται και αποθηκεύονται μέσω \en{PersistentDataset (cache)}, ενώ οι
στοχαστικοί μετασχηματισμοί επαύξησης εφαρμόζονται \en{on-the-fly}.

\section{Δειγματοληψία \en{patches} και επαύξηση δεδομένων}
\label{sec:exp_aug}

Λόγω του μεγάλου μεγέθους των \en{3D} όγκων και των περιορισμών μνήμης,
η εκπαίδευση πραγματοποιείται σε \en{patches} σταθερού μεγέθους
\en{$\texttt{roi\_size}$}.
Για την αντιμετώπιση της ισχυρής ανισορροπίας \en{background/foreground},
εφαρμόζεται \en{RandCropByPosNegLabeld} με ρύθμιση της αναλογίας
θετικών/αρνητικών \en{patches}.

Επιπλέον, χρησιμοποιείται \textbf{στρατηγική \en{curriculum}} στην αναλογία
\en{pos/neg} κατά τη διάρκεια της εκπαίδευσης:
\begin{itemize}
  \item \textbf{Στάδιο 1 (\en{epochs} 0--39):} \en{foreground-only} δειγματοληψία
        (\en{pos=1, neg=0}).
  \item \textbf{Στάδιο 2 (\en{epochs} 40--69):} \en{mixed} δειγματοληψία
        (\en{pos=3, neg=1}).
  \item \textbf{Στάδιο 3 (\en{epochs} 70+):} πιο ισορροπημένη δειγματοληψία
        (\en{pos=1, neg=1}).
\end{itemize}
Η σταδιακή εισαγωγή αρνητικών \en{patches} στοχεύει στη μείωση των
\en{false positives}, χωρίς να θυσιάζεται η ικανότητα του μοντέλου να “βλέπει”
αρκετό \en{foreground} στα πρώτα στάδια σύγκλισης.

Για βελτίωση της γενίκευσης εφαρμόζονται στοχαστικές επαυξήσεις όπως:
\begin{itemize}
  \item τυχαίες αναστροφές (\en{flip}) σε κάθε άξονα (\en{prob=0.5}),
  \item ήπιες περιστροφές (\en{range} $\approx 0.15$ \en{rad}) και
        αφινικοί μετασχηματισμοί (περιστροφή έως $\pi/6$ γύρω από $z$ και
        κλίμακα έως $\pm 0.2$),
  \item προσθήκη \en{Gaussian noise} (\en{std=0.02}) και
        \en{Gaussian smoothing} (τυπικές αποκλίσεις $\sigma\in[0.5,1.0]$),
\end{itemize}
ώστε να προσομοιώνεται μεταβλητότητα πρωτοκόλλων/θορύβου χωρίς αλλοίωση της
σημασιολογίας της μάσκας.

\section{Πρωτόκολλο εκπαίδευσης}
\label{sec:exp_training}

Η εκπαίδευση πραγματοποιείται για \en{$\texttt{epochs}=120$} με μικρό
\en{batch size} ($1$) λόγω μνήμης και χρήση \en{gradient accumulation}
(\en{\texttt{accum\_steps}=4}) ώστε να προσεγγίζεται μεγαλύτερο
αποτελεσματικό \en{batch}.

Οι κύριες επιλογές εκπαίδευσης είναι:
\begin{itemize}
  \item \textbf{Απώλεια:} \en{Dice + Cross-Entropy} μέσω \en{DiceCELoss},
        με \en{sigmoid} έξοδο και \en{include\_background=False}, ώστε να
        βελτιστοποιείται τόσο η επικάλυψη όσο και η σταθερότητα εκπαίδευσης.
        (Για ανάλυση, καταγράφονται επιπλέον τα επιμέρους \en{Dice-only} και
        \en{CE-only} σκέλη.)
  \item \textbf{Βελτιστοποιητής:} \en{AdamW} \cite{loshchilov2019adamw},
        με \en{weight decay} $2\cdot 10^{-5}$ και αρχικό
        \en{learning rate} \en{$\texttt{base\_lr}=2.5\cdot 10^{-5}$}.
  \item \textbf{\en{Scheduler:}} κυκλικός ρυθμός μάθησης
        \en{CyclicLR} \cite{smith2017cyclical} (\en{triangular2}) μεταξύ
        \en{$\texttt{base\_lr}$} και \en{$\texttt{max\_lr}=4\cdot 10^{-4}$}.
  \item \textbf{\en{Mixed precision}:} χρήση \en{AMP} για μείωση μνήμης και
        επιτάχυνση σε \en{GPU}.
  \item \textbf{\en{EMA}:} εκθετικός κινητός μέσος των βαρών
        (\en{decay=0.975}) για σταθερότερη αξιολόγηση.
  \item \textbf{Επιλογή καλύτερου μοντέλου / early stopping:} αποθήκευση
        του καλύτερου \en{checkpoint} με βάση το \en{Dice} στο
        \en{validation set} και διακοπή όταν δεν υπάρχει βελτίωση για
        \en{patience=30} αξιολογήσεις.
\end{itemize}

\section{Διαδικασία αξιολόγησης και μετρικές}
\label{sec:exp_metrics}

Η αξιολόγηση γίνεται σε επίπεδο \en{3D} όγκου.
Για την παραγωγή πρόβλεψης σε ολόκληρο τον όγκο χρησιμοποιείται
\en{sliding window inference} με \en{overlap=0.5} και \en{gaussian blending},
ώστε να είναι εφικτή η αξιολόγηση υπό περιορισμούς μνήμης.

Η κύρια μετρική αναφοράς είναι ο \textbf{συντελεστής \en{Dice}}
\cite{dice1945}:
\[
\mathrm{Dice}(P,G) = \frac{2|P\cap G|}{|P|+|G|},
\]
όπου $P$ η δυαδική πρόβλεψη και $G$ η δυαδική μάσκα αναφοράς.
Επιπλέον καταγράφεται το \textbf{\en{Intersection-over-Union (IoU/Jaccard)}}
\cite{jaccard1912}:
\[
\mathrm{IoU}(P,G) = \frac{|P\cap G|}{|P\cup G|}.
\]

Η δυαδικοποίηση πραγματοποιείται με κατώφλι (\en{threshold}).
Ως αρχική τιμή χρησιμοποιείται \en{$t=0.5$}, ενώ ανά τακτά διαστήματα
(\en{every 5 epochs}) εκτελείται \en{threshold sweep} σε πλέγμα τιμών
\en{$t\in[0.35,0.65]$} και επιλέγεται το βέλτιστο ως προς \en{Dice}.

Τέλος, εφαρμόζεται μετα-επεξεργασία με διατήρηση της μεγαλύτερης συνεκτικής
συνιστώσας (\en{KeepLargestConnectedComponent}), ώστε να μειωθούν
απομονωμένα \en{false positives} και να επιβληθεί ένας ρεαλιστικός
μορφολογικός περιορισμός (ενιαία κύρια περιοχή πλακούντα).

\section{Αρχιτεκτονικές και πειραματικά σενάρια}
\label{sec:exp_models}

Αξιολογήθηκαν οι παρακάτω αρχιτεκτονικές:
\begin{itemize}
  \item \en{U-Net} \cite{ronneberger2015unet}
  \item \en{Attention U-Net} \cite{oktay2018attentionunet}
  \item \en{DynUNet} \cite{isensee2021nnunet}
  \item \en{UNETR} \cite{hatamizadeh2022unetr}
  \item \en{SwinUNETR} \cite{hatamizadeh2022swinunetr}
  \item \en{SegResNet} \cite{myronenko2018autoencoder}
  \item \en{SegMamba} \cite{xing2024segmamba}
\end{itemize}

Για κάθε αρχιτεκτονική εκτελέστηκε μία βασική εκπαίδευση
(\en{baseline run}) με το κοινό \en{pipeline}.
Επιπλέον πραγματοποιήθηκαν ενισχυμένα πειράματα (\en{stronger runs}) για
επιλεγμένα μοντέλα (π.\,χ. \en{SegResNet} και \en{SwinUNETR}), με στόχο να
διερευνηθεί η επίδραση ρυθμίσεων που αυξάνουν την ικανότητα του δικτύου ή/και
βελτιώνουν τη σταθερότητα σύγκλισης (π.\,χ. μεταβολές σε
\en{feature size, window size}, βάθος, κ.\,ά.).
Οι αποκλίσεις αυτές από τη βασική ρύθμιση καταγράφονται ρητά στις αντίστοιχες
υποενότητες αποτελεσμάτων.

\section{Παρουσίαση αποτελεσμάτων}
\label{sec:exp_results}

Τα αποτελέσματα παρουσιάζονται σε δύο επίπεδα:
\begin{itemize}
  \item \textbf{Ποσοτικά:} μετρικές \en{Dice/IoU} και απώλειες
        (\en{train/val loss}), με σύγκριση μεταξύ μοντέλων.
  \item \textbf{Ποιοτικά:} ενδεικτικές οπτικοποιήσεις προβλέψεων/σφαλμάτων
        σε αντιπροσωπευτικά περιστατικά.
\end{itemize}

Ο Πίνακας~\ref{tab:main_results} συνοψίζει τη βασική επίδοση ανά μοντέλο στο
\en{validation set}. Η αναλυτική ερμηνεία, συγκριτική συζήτηση και
\en{error analysis} παρουσιάζονται στο Κεφάλαιο~\ref{ch:discussion}.

\begin{table}[h]
\centering
\caption{Σύνοψη αποτελεσμάτων ανά αρχιτεκτονική στο \en{validation set}.}
\label{tab:main_results}
\begin{tabular}{lccc}
\hline
\textbf{Μοντέλο} & \en{\textbf{Dice}} & \en{\textbf{IoU}} & \en{\textbf{Val Loss}} \\
\hline
\en{U-Net}            &  &  &  \\
\en{Attention U-Net}  &  &  &  \\
\en{DynUNet}          &  &  &  \\
\en{UNETR}            &  &  &  \\
\en{SwinUNETR}        &  &  &  \\
\en{SegResNet}        &  &  &  \\
\en{SegMamba}         &  &  &  \\
\hline
\end{tabular}
\end{table}