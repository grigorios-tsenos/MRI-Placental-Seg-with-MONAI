\chapter{Πειραματικό Πρωτόκολλο και Μεθοδολογία Αξιολόγησης}
\label{ch:experiments}

Στο κεφάλαιο αυτό παρουσιάζεται η πειραματική διαδικασία που θεσπίστηκε  
με σκοπό την εκπαίδευση και τη συγκριτική αξιολόγηση διαφορετικών αρχιτεκτονικών, 
με γνώμονα τη διεργασία της τμηματοποίησης σε \en{3D MRI} εικόνες.

Η επιλογή των μοντέλων προέκυψε έπειτα από συστηματική μελέτη της βιβλιογραφίας:
αρχικά εξετάστηκαν τα πρωτότυπα δίκτυα σχήματος \en{U} (\en{U-Net})
\cite{ronneberger2015unet} και στη συνέχεια παραλ\-λαγές/επεκτάσεις όπως το
\en{Attention U-Net} \cite{oktay2018attentionunet}, σύγχρονες \en{U-Net} μορφές
όπως το \en{DynUNet} \cite{isensee2021nnunet} και όμοιες \en{CNN-based encoder-decoder} μορφές 
που υλοποιούν υπολειμματικές συνδέ\-σεις (\en{SegResNet}) \cite{myronenko2018segresnet}.
Τέλος, αξιολογήθηκαν νεότερες προσεγγίσεις που μοντελοποιούν μακρινές
εξαρτήσεις, όπως τα \en{Transformer-based} μοντέλα
(\en{UNETR} \cite{hatamizadeh2022unetr},
\en{SwinUNETR} \cite{hatamizadeh2022swinunetr})
και \en{State Space-based} μοντέλα (\en{SegMamba} \cite{xing2024segmamba}, επέκταση του \en{Mamba}
\cite{gu2023mamba}).

Κεντρικός στόχος είναι η \textbf{δίκαιη και αναπαραγώγιμη σύγκριση} των
αρχιτεκτονικών. Για τον λόγο αυτό υλοποιήθηκε ένα \textbf{ενιαίο πειραματικό
πλαίσιο} όπου όλες οι αρχιτεκτονικές εκπαιδεύονται με κοινή ροή
(\en{pipeline}) προεπεξεργασίας, δειγματοληψίας \en{patches}, εκπαίδευσης,
αξιολόγησης και καταγραφής μετρικών.
Στην πράξη, κάθε αρχιτεκτονική αντι\-στοιχεί σε ξεχωριστό \en{notebook},
όμως η δομή του κώδικα παραμένει σταθερή, με κύρια διαφοροποίηση την
αρχικοποίηση του μοντέλου και ελάχιστες αναγκαίες ρυθμίσεις συμβατότητας.

\section{Στόχοι αξιολόγησης και αρχές σύγκρισης}
\label{sec:exp_principles}

Στόχος της πειραματικής αξιολόγησης είναι να αποτυπωθεί με σαφήνεια η
απόδοση διαφορετικών αρχιτεκτονικών στη τμηματοποίηση
πλακούντα σε \en{3D MRI}. Η σύγκριση σχεδιάστηκε ώστε να είναι δίκαιη, με
κοινό τρόπο αναφοράς αποτελεσμάτων και συνεπείς επιλογές αξιολόγησης, έτσι
ώστε οι παρατηρούμενες διαφορές να αποδίδονται όσο δυνατόν αποκλειστικά στην
αρχιτεκτονική.

Η αναφορά βασίζεται κυρίως σε \en{Dice} και \en{IoU}, μετρικές που είναι
κατάλληλες για προβλήματα έντονης ανισορροπίας κλάσεων, ενώ 
παράλληλα συμπληρώνεται από ποιοτική επιθεώρηση και
τυπική σύγκριση με αποτελέσματα απο άλλες έρευνες ομοίου σκοπού (βλ. Κεφάλαιο~\ref{ch:results}).

\section{Υλοποίηση και λογισμικό}
\label{sec:exp_software}

Τα πειράματα υλοποιήθηκαν κυρίως με χρήση των βιβλιοθηκών \en{PyTorch} και 
\en{MONAI} \cite{cardoso2022monai} σε γλώσσα \en{Python}. Η εκπαίδευση εκτελέστηκε σε περιβάλλον
\en{cloud} μέσω της πλατφόρμας \en{Kaggle Notebooks}, αξιοποιώντας \en{GPU} επιτάχυνση 
(\en{NVIDIA T4} σε διαθέσιμη διαμόρφωση) εντός του εβδομαδιαίου ορίου χρήσης που 
παρέχεται από την πλατφόρμα.\footnote{Οι ακριβείς διαθέσιμοι πόροι και τα όρια χρήσης μπορεί να
μεταβάλλονται με τον χρόνο και εξαρτώνται από την πολιτική της πλατφόρμας.}

Οι εκτελέσεις πραγματοποιήθηκαν ως \emph{\en{non-interactive runs}} 
(με καταγραφή \en{logs}/μετρικών), ώστε να διατηρείται σταθερή
η διαδικασία μεταξύ μοντέλων και να αποφεύγονται χειροκίνητες παρεμβάσεις.
Για αναπαραγωγιμότητα χρησιμοποιήθηκαν σταθερά \en{seeds} σε 
\en{NumPy} και \en{PyTorch}, καθώς και κοινή δομή κώδικα για όλα τα μοντέλα
(με διαφοροποίηση κυρίως στην αρχικοποίηση της αρχιτεκτονικής).

\section{Σύνολο δεδομένων και διαχωρισμός}
\label{sec:exp_data}

Το σύνολο δεδομένων περιλαμβάνει $N=137$ περιστατικά, με ένα ζεύγος αρχείων
\en{\texttt{.nii.gz}} ανά περίπτωση: την εικόνα καταγραφής του ασθενή στον Μαγνητικό Τομογράφο
και την αντίστοιχη δυαδική μάσκα τμηματοποί\-ησης πλακούντα.
Η επέκταση αρχείου \en{nii} προέρχεται απο την λέξη \en{NIfTI} όπου είναι ακρωνύμιο των λέξεων 
\en{\textbf{N}euroimaging \textbf{I}n\textbf{f}orfmatics \textbf{T}echnology \textbf{I}nitiative}. 
Με αυτή τη δομή μαζι με την εικόνα αποθηκεύονται και στατιστικά, ρυθμίσεις φωτισμού και 
διάφορες άλλες χρήσιμες πληροφορίες για την εικόνα (\en{metadata}).

Ως βασική μονάδα εκπαίδευσης και αξιολόγησης ορίζεται το \en{case}
(δηλαδή ολόκληρος ο \en{3D} όγκος), ώστε ο διαχωρισμός να γίνεται σε επίπεδο
ασθενή/περίπτωσης και να αποφεύγεται διαρροή πληροφορίας.

Ο διαχωρισμός σε σύνολα εκπαίδευσης και επικύρωσης πραγματοποιείται με
σταθερό \en{seed} ($121$) για λόγους αναπαραγωγιμότητας.
Στα πειράματα της παρούσας εργασίας χρησιμοποιήθηκε διαχωρισμός $80/20$,
που αντιστοιχεί περίπου σε $109$ περιστατικά εκπαίδευσης και $28$ επικύρωσης \en{(validation)}.

\section{Οργάνωση δεδομένων και ροή φόρτωσης}
\label{sec:proto_dataflow}
Κάθε περιστατικό περιλαμβάνει έναν \en{3D MRI} όγκο και την αντίστοιχη
δυαδική μάσκα πλακούντα σε μορφή \en{NIfTI (\texttt{.nii})}. Τα
δεδομένα μετατρέπονται σε λίστα από \en{dictionaries} (το καθένα περιέχει ένα κλειδί για
\en{\texttt{image}} με τιμή το μονοπάτι προς την εικόνα και ένα κλειδί \en{\texttt{label}} 
αντίστοιχα με τιμή το μονοπάτι προς την μάσκα), η οποία τροφοδοτείται σε
\en{Dataset} του \en{MONAI}.

Για επιτάχυνση, το αιτιοκρατικό τμήμα της προεπεξεργασίας υπολογίζεται μια φορά και αποθηκεύεται σε \en{disk cache} μέσω \textbf{\en{PersistentDataset}}.
Αντιθέτως, οι στοχαστικοί μετασχηματισμοί (\en{Random Augmentations}, εφαρμόζονται μόνο στο \en{training set}) εκτελούνται κάθε εποχή κατά τη φόρτωση των δεδομένων μέσω \en{DataLoader}. 
Αυτό συμβαίνει ώστε να έχουμε διαφορετικές παραλλαγές των ίδιων δειγμάτων χωρίς \textbf{επαναλαμβανόμενο} κόστος σταθερής προεπεξεργασίας.

\section{Ανάλυση στοίβας προεπεξεργασίας}
\label{sec:proto_preproc}

Η στοίβα προεπεξεργασίας σχεδιάστηκε έχοντας υπόψη δύο βασικά σημεία: 
(α) υψηλό \en{background to foreground ratio} 
(ο πλακούντας καταλαμβάνει μικρό μέρος του όγκου) και (β) την ανάγκη
γεωμετρικής συνέπειας μεταξύ περιστατικών. Οι μετασχηματισμοί επιλέχθηκαν
από το \en{MONAI} \cite{monaitransforms}.

\subsection{Γεωμετρική εναρμόνιση και σταθεροποίηση εισόδου}
Το πρώτο στάδιο εξασφαλίζει ότι το μοντέλο διαβάζει τα δεδομένα σε κοινό γεωμετρικό πλαίσιο:
\begin{itemize}
  \item \textbf{Φόρτωση και τυποποίηση:} ανάγνωση \en{NIfTI}, μεταφορά σε
        \en{tensor} και εξασφάλιση σωστού καναλιού (\en{channel-first}),
        ώστε οι επόμενοι μετασχηματισμοί να εφαρμόζονται ομοιόμορφα.
  \item \textbf{Επαναπροσανατολισμός (\en{Orientation}):} μετατροπή σε κοινό
        σύστημα αξόνων (\en{RAS}), ώστε να αποφεύγονται ασυνέπειες σε
        \en{left/right, anterior/posterior} κ.λπ.
  \item \textbf{Αναδειγματοληψία (\en{Spacing}):} μετασχηματισμός σε κοινό
        \en{voxel spacing ($(2.0,2.0,2.0)$\, mm)}, ώστε η μάθηση να μην
        επηρεάζεται από διαφορετικές φυσικές κλίμακες και ανομοιογενείς
        αναλύσεις μεταξύ εξετάσεων.
\end{itemize}

\subsection{Κανονικοποίηση εντάσεων}
Εφαρμόζεται κανονικοποίηση με \en{percentiles} (π.χ. 2.0-99.9) και
χαρτογράφηση στο $[0,1]$. Η πρακτική αυτή (α) περιορίζει την επίδραση
\en{outliers}/θορύβου και (β) διευκολύνει τη σταθερότητα του \en{optimization}.

\subsection{Περιορισμός πεδίου (\en{ROI}) με \en{foreground cropping}}
Δεδομένης της μεγάλης έκτασης \en{background}, εφαρμόστηκε περικοπή σε πλαίσιο με την 
\en{CropForegroundd} με \en{source\_key=label} και μικρό περιθώριο (\en{margin}$=8$) ώστε να
περιορίζεται η εικόνα γύρω από την περιοχή ενδιαφέροντος. Με τον τρόπο
αυτό μειώνεται δραστικά το μέγεθος του υπολογιστικού κόστους και παράλληλα αυξάνοντας
την πιθανότητα τα \en{patches} να περιέχουν χρήσιμη πληροφορία.

\subsection{Σταθεροποίηση διαστάσεων για συμβατότητα μοντέλων}
Τέλος, εφαρμόζεται:
\begin{itemize}
  \item \textbf{\en{Padding} σε σταθερό \en{roi\_size}} ($(96,96,64)$), για
        ομοιόμορφη εκπαίδευση με \en{patch-based} στρατηγική.  
        \item \textbf{\en{Divisible padding}} ώστε οι τελικές διαστάσεις να είναι
        πολλαπλάσια συγκεκριμένων παραγόντων (π.χ. $(32,32,16)$), κάτι που
        διευκολύνει αρχιτεκτονικές με \en{downsampling/patch merging}.
\end{itemize}

\section{Δειγματοληψία \en{patches} και επαύξηση δεδομένων}
\label{sec:proto_patches_aug}

Λόγω του μεγέθους των \en{3D} όγκων και περιορισμών μνήμης, η εκπαίδευση
πραγματοποιείται σε \en{patches} σταθερού μεγέθους (\en{roi\_size}$=(96,96,64)$).

Για την αντιμετώπιση της έντονης ανισορροπίας \en{background/foreground}
χρησιμοποιή\-θηκε \en{RandCropByPosNegLabeld}, ώστε να ελέγχεται η πιθανότητα
επιλογής \en{patches} που περιέχουν \en{foreground}. Επιπλέον εφαρμόστηκε μεταβολή στην αναλογία
\en{pos/neg} κατά τη διάρκεια της εκπαίδευσης, με στόχο αρχικά να
σταθεροποιηθεί η μάθηση πάνω στο \en{foreground} και στη συνέχεια να
μειωθούν τα \en{false positives} μέσω σταδιακής εισαγωγής αρνητικών
δειγμά\-των. Για τα πρώτα 40 \en{epochs} είχαμε αποκλειστικά \en{foreground} 
δεδομένα, έπειτα για 30 \en{epochs} υπήρχε αναλογία $3/1$ \en{foreground to background} δεδομένα, 
και για τις υπόλοιπες εποχές ισάξια αναλογία θετικών προς αρνητικών δειγμάτων. 

Για βελτίωση της γενίκευσης, εφαρμόστηκαν στοχαστικές επαυξήσεις
(\en{random flips}, ήπιες περιστροφές, διαστρεβλώσεις όπως \en{stretching} κ.λ.π., \en{Gaussian}
θόρυβος/εξομάλυνση), οι οποίες προσομοιώνουν ρεαλιστική μεταβλητότητα χωρίς
να αλλοιώνουν τη σημασιολογία της μάσκας.

\section{Αρχιτεκτονικές και πειραματικά σενάρια}
\label{sec:exp_models}

Αξιολογήθηκαν οι παρακάτω αρχιτεκτονικές:
\begin{itemize}
  \item \en{U-Net} \cite{ronneberger2015unet}
  \item \en{Attention U-Net} \cite{oktay2018attentionunet}
  \item \en{DynUNet} \cite{isensee2021nnunet}
  \item \en{UNETR} \cite{hatamizadeh2022unetr}
  \item \en{SwinUNETR} \cite{hatamizadeh2022swinunetr}
  \item \en{SegResNet} \cite{myronenko2018segresnet}
  \item \en{SegMamba} \cite{xing2024segmamba}
\end{itemize}

\paragraph{Οικογένειες μοντέλων.}
Οι αρχιτεκτονικές που αξιολογούνται καλύπτουν κλασικές \en{CNN} προσεγγίσεις
(\en{U-Net} και παραλλαγές), νεότερες \en{Transformer-based} δομές
(\en{UNETR, SwinUNETR}) και αποδοτικές εναλλακτικές για μοντελοποίηση
μακρινών εξαρτήσεων (\en{state space-based}, \en{SegMamba}). Με αυτόν τον τρόπο
εξετάζεται τόσο η επίδραση ισχυρής τοπικής επαγωγικής μεροληψίας (\en{inductive bias}) όσο και η
συμβολή μηχανισμών καθολικών συμφραζομένων (\en{global context}).

\subsection{Αρχιτεκτονικές υπερπαράμετροι ανά μοντέλο}
\label{subsec:arch_hparams}

Παρότι το πειραματικό πρωτόκολλο (προεπεξεργασία, δειγματοληψία, \en{optimizer}, κ.λπ.)
διατηρήθηκε κοινό, κάθε αρχιτεκτονική απαιτεί ορισμένες \textit{ειδικές}
υπερπαραμέτρους που καθορίζουν κυρίως τη χωρητικότητα (\en{capacity}) και τη δομή
του \en{encoder/decoder}. Το \en{dropout} ορίστηκε $0.0$ σε κάθε περίπτωση. Συνοπτικά, χρησιμοποιήθηκαν:

\begin{itemize}
  \item \textbf{\en{U-Net}:}
  \en{channels} $(16,32,64,128,256)$, \en{strides} $(2,2,2,2)$,
  \en{num\_res\_units} $=2$.

  \item \textbf{\en{Attention U-Net}:}
  \en{channels} $(16,32,64,128,256)$, \en{strides} $(2,2,2,2)$,
  \en{kernel\_size} $=3$, \en{up\_kernel\_size} $=3$.

  \item \textbf{\en{DynUNet}:}
  \en{kernel\_size} $=[3,3,3,3]$, \en{strides} $=[1,2,2,2]$,
  \en{upsample\_kernel\_size} $=[2,2,2]$.

  \item \textbf{\en{UNETR}:}
  \en{img\_size} $(96,96,64)$, \en{feature\_size} $=32$,
  \en{hidden\_size} $=768$, \en{mlp\_dim} $=3072$, \en{num\_heads} $=24$,
  \en{proj\_type}=\en{\texttt{"conv"}}, \en{res\_block}=\en{\texttt{True}}.

  \item \textbf{\en{SwinUNETR}:}
  εκτελέστηκαν δύο ρυθμίσεις:
  \begin{itemize}
    \item \en{SwinUNETR Heavier}: \en{img\_size} $(96,96,64)$,
    \en{feature\_size} $=60$, \en{patch\_size} $=2$, \en{window\_size} $=5$,
    \en{depths} $(2,2,2,2)$, \en{use\_checkpoint}=\en{\texttt{True}}.
    \item \en{SwinUNETR Lighter}: \en{img\_size} $(96,96,64)$,
    \en{feature\_size} $=48$, \en{patch\_size} $=2$, \en{window\_size} $=4$,
    \en{depths} $(2,2,2,2)$, \en{use\_checkpoint}=\en{\texttt{True}}.
  \end{itemize}

  \item \textbf{\en{SegResNet}:}
  βασικές παράμετροι όπως προαρμογή πλάτους επιπέδων (\en{init\_filters}) και η κατανομή \en{blocks}
  στο \en{down/up path}. Εκτελέστηκαν δύο ρυθμίσεις:
  \begin{itemize}
    \item \en{SegResNet Heavier}: \en{init\_filters}$=64$,
    \en{blocks\_down}$=[1,2,2,4]$, \en{blocks\_up}$=[1,1,1]$.
    \item \en{SegResNet Lighter}: \en{init\_filters}$=32$,
    \en{blocks\_down}$=[1,2,2,4]$, \en{blocks\_up}$=[1,1,1]$.
  \end{itemize}

  \item \textbf{\en{SegMamba}:}
  βασικές παράμετροι το ``πλάτος'' χαρακτηριστικών (\en{feat\_size}) και το βάθος
  (\en{depths}). Εκτελέστηκαν δύο ρυθμίσεις:
  \begin{itemize}
    \item \en{SegMamba Heavier}: \en{feat\_size}$=32$, \en{depths}$=[2,2,2,2,2]$.
    \item \en{SegMamba Lighter}: \en{feat\_size}$=32$, \en{depths}$=[2,2,2,2]$.
  \end{itemize}
\end{itemize}

Οι παραπάνω ρυθμίσεις αφορούν αποκλειστικά τη \textbf{χωρητικότητα της αρχιτεκτονικής}
και όχι το εκπαιδευτικό πρωτόκολλο, το οποίο παρέμεινε κοινό για όλα τα μοντέλα.

Για κάθε αρχιτεκτονική εκτελέστηκε μία βασική εκπαίδευση
(\en{baseline run}) με το κοινό \en{pipeline}.
Επιπλέον πραγματοποιήθηκαν πειράματα εναλλαγής χωρητικότητας για επιλεγμένα μοντέλα (\en{SegResNet}, \en{SegMamba} και \en{SwinUNETR}), με στόχο να
διερευνηθεί η επίδραση ρυθμίσεων που αυξάνουν την ικανότητα του δικτύου ή/και
βελτιώνουν τη σταθερότητα σύγκλισης.

Το ενισχυμένο πείραμα του \en{SegResNet} είχε μεγαλύτερο πλάτος το δίκτυο, 
ενώ του \en{SegMamba} μεγαλύτερο βάθος (περισσότερα \en{layers}).
Για το \en{SwinUNETR} συγκρίθηκαν η αρχική ρύθμιση (\en{Heavier}) και μία
ελαφρύτερη παραλλαγή (\en{Lighter}) με μικρότερα \en{feature\_size} και
\en{window\_size}.
Οι αποκλίσεις αυτές από τη βασική ρύθμιση καταγράφονται ρητά στις αντίστοιχες
υποενότητες αποτελεσμάτων (\ref{sec:results_ranking} - \ref{sec:results_qual}, \ref{par:costvacc}).

\section{Συνάρτηση κόστους και βελτιστοποίηση}
\label{sec:proto_scheduler}

\subsection{Κριτήριο εκπαίδευσης}
Ως βασικό κριτήριο χρησιμοποιήθηκε η \en{DiceCELoss} (\en{Dice + Cross-Entropy}),
με \en{sigmoid} έξοδο και \en{include\_background=False}. Ο συνδυασμός
\en{Dice} και \en{CE} είναι πρακτικός σε έντονα ανισόρροπες τμηματοποιήσεις,
καθώς ο \en{Dice} στοχεύει άμεσα στην επικάλυψη, ενώ το \en{CE} συνεισφέρει
σταθερότητα και καλύτερη συμπεριφορά gradients σε πρώιμα στάδια σύγκλισης.

\section{Επικύρωση και διαδικασία \en{inference}}
\label{sec:proto_validation}

\subsection{\en{Sliding Window Inference (SWI)}}
Η επικύρωση σε πλήρεις \en{3D} όγκους είναι χρονοβόρα αλλά και ακριβή σε μνήμη, ειδικά για
βαριά μοντέλα. Για τον λόγο αυτό εφαρμόστηκε \en{sliding-window inference}
με επικάλυψη (\en{overlap}) και \en{gaussian blending}, ώστε να συντίθεται
τελική πρόβλεψη σε όλο τον όγκο χωρίς να θυσιάζεται η ποιότητα στα όρια των
\en{patches}.

\subsection{\en{EMA (Exponential Moving Average)}}
Παράλληλα, χρησιμοποιήθηκε \en{EMA}, δηλαδή εκθετικός κινητός μέσος
των παραμέτρων του μοντέλου. Η αξιολόγηση με \en{EMA} τείνει να είναι πιο
σταθερή από το στιγμιαίο \en{checkpoint}, ειδικά όταν ο ρυθμός μάθησης κυμαίνεται σε εύρος.

\section{Βελτιστοποίηση και \en{Scheduler}}
\label{sec:proto_optim}

\subsection{\en{Optimizer}}
Για όλες τις υλοποιήσεις χρησιμοποιήθηκε ο \en{AdamW} με τις ίδιες ρυθμίσεις.

Στα πειράματα τέθηκαν \en{\texttt{base\_lr = 2.5e-5}} και
\en{\texttt{weight\_decay = 2e-5}}. Η τιμή \en{\texttt{lr=base\_lr}} λειτουργεί ως
βασικός ρυθμός μάθησης, πάνω στον οποίο εφαρμόζεται ο \en{scheduler}
της επόμενης υποενότητας.

Οι παράμετροι \en{betas=(0.9, 0.999)} αντιστοιχούν στους συντελεστές
$\beta_1$ και $\beta_2$ του \en{Adam}-τύπου κινητού μέσου: η
$\beta_1=0.9$ ελέγχει την εξομάλυνση της πρώτης ροπής (μέση κλίση, τύπου
\en{momentum}), ενώ η $\beta_2=0.999$ την εξομάλυνση της δεύτερης ροπής
(μέσο τετραγωνικών κλίσεων). 

Οι συγκεκριμένες επιλογές ήταν σταθερές σε όλες
τις αρχιτεκτονικές για δίκαιη σύγκριση. Η θεωρητική ανάλυση του
\en{AdamW} δίνεται στην Ενότητα~\ref{sec:training_practices}.

\subsection{\en{Scheduler}}
Για λόγους αποφυγής τοπικών ελαχίστων και άλλους, 
έγινε χρήση \en{Scheduler} ώστε ο ρυθμός μάθησης να (\en{learning rate}) μεταβαλλόταν 
διαρκώς κατά τη διάρκεια της
εκπαίδευσης με \en{CyclicLR} \cite{smith2017cyclical} (σχήμα \en{triangular2}).
Στο συγκεκριμένο σχήμα ο ρυθμός μάθησης αυξάνεται γραμμικά από
\en{\texttt{base\_lr}} έως \en{\texttt{max\_lr}} και στη συνέχεια μειώνεται πάλι, ενώ σε κάθε
επόμενο κύκλο το εύρος ταλάντωσης μειώνεται, οδηγώντας σε πιο «ήπια» εξερεύνηση στα
όψιμα στάδια. 

Η επιλογή κυκλικού χρονοπρογραμματισμού εξυπηρετεί δύο στόχους:
(\en{i}) μειώνει την ευαισθησία σε μία μοναδική επιλογή \en{learning rate}, και
(\en{ii}) λειτουργεί ως μηχανισμός εξερεύνησης του χώρου λύσεων, κάτι που συχνά βοηθά
σε πιο σταθερή σύγκλιση όταν συνδυάζεται με έντονες στοχαστικές διεργασίες
(όπως \en{data augmentation} και \en{patch sampling}).

Σημειώνεται ότι στα διαγράμματα ιστορικού εκπαίδευσης αποτυπώνεται ο κυκλικός
χαρακτήρας του \en{LR}, καθώς και η χρονική στιγμή στην οποία επιτυγχάνεται η
καλύτερη επίδοση στο \en{validation} (π.χ. μέγιστο \en{Dice}), που χρησιμοποιείται
και ως κριτήριο επιλογής \en{checkpoint}.


\clearpage
