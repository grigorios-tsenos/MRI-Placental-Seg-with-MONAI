\chapter{Πειραματικό Πρωτόκολλο και Υλοποίηση}
\label{ch:protocol}

Το κεφάλαιο αυτό περιγράφει αναλυτικά το πειραματικό πρωτόκολλο που
ακολουθήθηκε για την εκπαίδευση και αξιολόγηση των μοντέλων τμηματοποίησης.
Στόχος είναι η τεκμηρίωση ενός ενιαίου και αναπαραγώγιμου πλαισίου
(προεπεξεργασία, δειγματοληψία, εκπαίδευση, επικύρωση), ώστε οι διαφορές
στα αποτελέσματα του Κεφαλαίου~\ref{ch:experiments} να αποδίδονται κατά το δυνατόν στην αρχιτεκτονική και όχι σε ασυνεπείς ρυθμίσεις.

\section{Υπολογιστικό περιβάλλον και εκτέλεση πειραμάτων}
\label{sec:proto_env}
Τα πειράματα υλοποιήθηκαν σε \en{Python/PyTorch} με τη βιβλιοθήκη
\en{MONAI} \cite{cardoso2022monai}. Η εκπαίδευση εκτελέστηκε σε περιβάλλον
\en{cloud} μέσω της πλατφόρμας \en{Kaggle Notebooks} (με επαλήθευση μέσω
τηλεφώνου), αξιοποιώντας \en{GPU} επιτάχυνση (π.χ. \en{NVIDIA T4} σε διαθέσιμη
διαμόρφωση) εντός του εβδομαδιαίου ορίου χρήσης που παρέχεται από την
πλατφόρμα.\footnote{Οι ακριβείς διαθέσιμοι πόροι και τα όρια χρήσης μπορεί να
μεταβάλλονται με τον χρόνο και εξαρτώνται από την πολιτική της πλατφόρμας.}

Οι εκτελέσεις πραγματοποιήθηκαν ως \emph{non-interactive runs} (\en{batch-style}
εκπαίδευση με καταγραφή \en{logs}/μετρικών), ώστε να διατηρείται σταθερή
η διαδικασία μεταξύ μοντέλων και να αποφεύγονται χειροκίνητες παρεμβάσεις.
Για αναπαραγωγιμότητα χρησιμοποιήθηκαν σταθερά \en{seeds} σε \en{Python},
\en{NumPy} και \en{PyTorch}, καθώς και κοινή δομή κώδικα για όλα τα μοντέλα
(με διαφοροποίηση κυρίως στην αρχικοποίηση της αρχιτεκτονικής).

\section{Οργάνωση δεδομένων και ροή φόρτωσης}
\label{sec:proto_dataflow}

Κάθε περιστατικό περιλαμβάνει έναν \en{3D MRI} όγκο και την αντίστοιχη
δυαδική μάσκα πλακούντα σε μορφή \en{NIfTI} (\en{\texttt{.nii.gz}}). Τα
δεδομένα μετατρέπονται σε λίστα από \en{dictionaries} (\en{paths} για
\en{\texttt{image}} και \en{\texttt{label}}), η οποία τροφοδοτείται σε
\en{Dataset} του \en{MONAI}.

Για επιτάχυνση, το αιτιοκρατικό τμήμα της προεπεξεργασίας προϋπολογίζεται και
αποθηκεύεται με \en{PersistentDataset (cache)}, ενώ οι στοχαστικοί
μετασχηματισμοί (η οποία εφαρμόζεται μονο στα δεδομένα εκπαίδευσης) επαύξησης εφαρμόζονται \en{on-the-fly} κατά τη φόρτωση μέσω
\en{DataLoader}. Η επιλογή αυτή μειώνει το συνολικό κόστος \en{I/O} και
εξασφαλίζει συνεπή επεξεργασία σε όλα τα \en{runs}.

\section{Ανάλυση στοίβας προεπεξεργασίας}
\label{sec:proto_preproc}

Η προεπεξεργασία σχεδιάστηκε με γνώμονα δύο βασικά χαρακτηριστικά του
προβλήματος: (α) την υψηλή ανισορροπία \en{background/foreground}
(ο πλακούντας καταλαμβάνει μικρό μέρος του όγκου) και (β) την ανάγκη
γεωμετρικής συνέπειας μεταξύ περιστατικών. Οι μετασχηματισμοί επιλέχθηκαν
από το \en{MONAI} \cite{monaitransforms}.

\subsection{Γεωμετρική εναρμόνιση και σταθεροποίηση εισόδου}
Το πρώτο στάδιο εξασφαλίζει ότι όλα τα δεδομένα «βλέπονται» από το μοντέλο
σε κοινό γεωμετρικό πλαίσιο:
\begin{itemize}
  \item \textbf{Φόρτωση και τυποποίηση:} ανάγνωση \en{NIfTI}, μεταφορά σε
        \en{tensor} και εξασφάλιση σωστού καναλιού (\en{channel-first}),
        ώστε οι επόμενοι μετασχηματισμοί να εφαρμόζονται ομοιόμορφα.
  \item \textbf{Επαναπροσανατολισμός (\en{Orientation}):} μετατροπή σε κοινό
        σύστημα αξόνων (\en{RAS}), ώστε να αποφεύγονται ασυνέπειες σε
        left/right, anterior/posterior κ.λπ.
  \item \textbf{Αναδειγματοληψία (\en{Spacing}):} μετασχηματισμός σε κοινό
        \en{voxel spacing} (π.χ. $(2,2,2)$\,mm), ώστε η μάθηση να μην
        επηρεάζεται από διαφορετικές φυσικές κλίμακες και ανομοιογενές
        \en{resolution} μεταξύ εξετάσεων.
\end{itemize}

\subsection{Κανονικοποίηση εντάσεων}
Οι \en{MRI} εντάσεις δεν είναι άμεσα συγκρίσιμες όπως σε \en{CT}, συνεπώς
εφαρμόζεται κανονικοποίηση με \en{percentiles} (π.χ. 2--99.9) και
χαρτογράφηση στο $[0,1]$. Η πρακτική αυτή (α) περιορίζει την επίδραση
outliers/θορύβου και (β) διευκολύνει τη σταθερότητα του optimization.

\subsection{Περιορισμός πεδίου (ROI) με \en{foreground cropping}}
Δεδομένης της μεγάλης έκτασης \en{background}, εφαρμόστηκε \en{CropForegroundd}
με \en{source\_key=label} και μικρό περιθώριο (\en{margin}) ώστε να
περιορίζεται το \en{FOV} γύρω από την περιοχή ενδιαφέροντος. Με τον τρόπο
αυτό μειώνεται δραστικά η σπατάλη υπολογισμού σε κενές περιοχές, αυξάνοντας
την πιθανότητα τα \en{patches} να περιέχουν χρήσιμη πληροφορία.

\subsection{Σταθεροποίηση διαστάσεων για συμβατότητα μοντέλων}
Τέλος, εφαρμόζεται:
\begin{itemize}
  \item \textbf{Padding σε σταθερό \en{roi\_size}} (π.χ. $(96,96,64)$), για
        ομοιόμορφη εκπαίδευση με \en{patch-based} στρατηγική.
  \item \textbf{\en{Divisible padding}} ώστε οι τελικές διαστάσεις να είναι
        πολλαπλάσια συγκεκριμένων παραγόντων (π.χ. $(32,32,16)$), κάτι που
        διευκολύνει αρχιτεκτονικές με downsampling/patch merging.
\end{itemize}

\section{Δειγματοληψία \en{patches} και επαύξηση δεδομένων}
\label{sec:proto_patches_aug}

Λόγω του μεγέθους των \en{3D} όγκων και περιορισμών μνήμης, η εκπαίδευση
πραγματοποιείται σε \en{patches} σταθερού μεγέθους (\en{roi\_size}).
Για την αντιμετώπιση της έντονης ανισορροπίας \en{background/foreground}
χρησιμοποιήθηκε \en{RandCropByPosNegLabeld}, ώστε να ελέγχεται η πιθανότητα
επιλογής \en{patches} που περιέχουν \en{foreground}.

Επιπλέον εφαρμόστηκε \textbf{στρατηγική \en{curriculum}} στην αναλογία
\en{pos/neg} κατά τη διάρκεια της εκπαίδευσης, με στόχο αρχικά να
σταθεροποιηθεί η μάθηση πάνω στο \en{foreground} και στη συνέχεια να
μειωθούν τα \en{false positives} μέσω σταδιακής εισαγωγής αρνητικών
δειγμάτων.

Για βελτίωση της γενίκευσης, εφαρμόστηκαν στοχαστικές επαυξήσεις
(\en{random flips}, ήπιες περιστροφές/αφινικοί μετασχηματισμοί, \en{Gaussian}
θόρυβος/εξομάλυνση), οι οποίες προσομοιώνουν ρεαλιστική μεταβλητότητα χωρίς
να αλλοιώνουν τη σημασιολογία της μάσκας.

\section{Συνάρτηση κόστους και βελτιστοποίηση}
\label{sec:proto_optim}

\subsection{Κριτήριο εκπαίδευσης}
Ως βασικό κριτήριο χρησιμοποιήθηκε η \en{DiceCELoss} (\en{Dice + Cross-Entropy}),
με \en{sigmoid} έξοδο και \en{include\_background=False}. Ο συνδυασμός
\en{Dice} και \en{CE} είναι πρακτικός σε έντονα ανισόρροπες τμηματοποιήσεις,
καθώς ο \en{Dice} στοχεύει άμεσα στην επικάλυψη, ενώ το \en{CE} συνεισφέρει
σταθερότητα και καλύτερη συμπεριφορά gradients σε πρώιμα στάδια σύγκλισης.

\subsection{Optimizer}
Χρησιμοποιήθηκε \en{AdamW} \cite{loshchilov2019adamw}, λόγω καλής πρακτικής
συμπεριφοράς σε \en{deep} δίκτυα και πιο ορθού χειρισμού του \en{weight decay}
σε σχέση με το κλασικό \en{Adam}. Οι υπερπαράμετροι (\en{learning rate, weight
decay}) διατηρήθηκαν κοινές μεταξύ μοντέλων όπου ήταν εφικτό, για δίκαιη
σύγκριση.



\subsection{Scheduler}
Ο ρυθμός μάθησης μεταβαλλόταν με \en{CyclicLR (triangular2)}
\cite{smith2017cyclical}, εντός ενός εύρους $[\texttt{\en{base\_lr}}, \texttt{\en{max\_lr}}]$.
Η κυκλική μεταβολή λειτουργεί ως μηχανισμός εξερεύνησης του \en{optimization
landscape}, μειώνοντας την ευαισθησία σε μία «μοναδική» επιλογή \en{learning rate}
και συχνά οδηγεί σε πιο σταθερή βελτίωση σε μεσαία/όψιμα στάδια.

\section{Επικύρωση και διαδικασία \en{inference}}
\label{sec:proto_validation}

\subsection{\en{Sliding Window Inference (SWI)}}
Η επικύρωση σε πλήρεις \en{3D} όγκους είναι συχνά αδύνατη σε μνήμη, ειδικά για
βαριά μοντέλα. Για τον λόγο αυτό εφαρμόστηκε \en{sliding-window inference}
με επικάλυψη (\en{overlap}) και \en{gaussian blending}, ώστε να συντίθεται
τελική πρόβλεψη σε όλο τον όγκο χωρίς να θυσιάζεται η ποιότητα στα όρια των
\en{patches}.

\subsection{EMA (Exponential Moving Average)}
Παράλληλα, χρησιμοποιήθηκε \en{EMA} των βαρών, δηλαδή εκθετικός κινητός μέσος
των παραμέτρων του μοντέλου. Η αξιολόγηση με \en{EMA} τείνει να είναι πιο
σταθερή από το στιγμιαίο \en{checkpoint}, ειδικά σε σχήματα με κυμαινόμενο
\en{learning rate}.

\subsection{Καταγραφή μετρικών και επιλογή \en{checkpoint}}
Η αξιολόγηση καταγράφει \en{Dice} (κύρια μετρική) και \en{IoU} σε επίπεδο
\en{3D} περιστατικού. Το καλύτερο checkpoint επιλέγεται βάσει της επίδοσης
στο \en{validation set}, ώστε να διατηρείται συνεπές σημείο αναφοράς μεταξύ
μοντέλων.

\section{Ρυθμίσεις εκπαίδευσης και σταθεροποίηση σύγκλισης}
\label{sec:proto_training_details}

Η εκπαίδευση πραγματοποιήθηκε για προκαθορισμένο αριθμό \en{epochs} με μικρό
\en{batch size} λόγω περιορισμών μνήμης. Για να προσεγγιστεί μεγαλύτερο
αποτελεσματικό \en{batch}, χρησιμοποιήθηκε \en{gradient accumulation}.
Παράλληλα, εφαρμόστηκε \en{mixed precision} (\en{AMP}) για μείωση της
κατανάλωσης \en{VRAM} και επιτάχυνση της εκτέλεσης σε \en{GPU}.

Για την αποφυγή υπερπροσαρμογής και τη συνεπή επιλογή τελικού μοντέλου,
αποθηκευόταν το καλύτερο \en{checkpoint} σύμφωνα με τη μετρική \en{Dice}
στο \en{validation set}, ενώ χρησιμοποιήθηκε \en{early stopping} όταν δεν
παρατηρούνταν βελτίωση για προκαθορισμένη \en{patience}.