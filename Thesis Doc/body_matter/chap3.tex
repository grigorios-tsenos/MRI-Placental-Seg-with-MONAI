% Chapter 3
\chapter{Μηχανική Μάθηση και Σύγχρονα Μοντέλα για Τμηματοποίηση}
\label{ch:ml_dl_models}

\InitialCharacter{Η} αυτόματη τμηματοποίηση με σύγχρονες προσεγγίσεις βασίζεται κυρίως σε \textbf{επιβλεπόμενη} Μηχανική Μάθηση,
όπου το μοντέλο εκπαιδεύεται σε ζεύγη ιατρικών εικόνων και των αντίστοιχων \textbf{μασκών τμηματοποίησης} που έχουν δημιουργηθεί χειροκίνητα απο ειδικούς \cite{litjens2017survey}.
Η επιτυχία της Βαθιάς Μάθησης στην τμηματοποίηση
οφείλεται στην ικανότητα της να μαθαί\-νει πλούσιες ιεραρχικές αναπαραστάσεις (απο απλά τοπικά χαρακτηριστικά έως πολύπλοκα χωρικά μοτίβα) από μεγάλα σύνολα δεδομένων, καθώς και στην
αποδοτική βελτιστοποίηση με \en{backpropagation}. Για γενικότερο πλαίσιο στη Βαθιά Μάθηση, παραπέμπουμε σε
κλασικές πηγές \cite{litjens2017survey, goodfellow2016deep, bishop2006pattern}.

\section{Επιβλεπόμενη μάθηση για τμηματοποίηση}
\label{sec:supervised_seg}

Έστω ότι διαθέτουμε σύνολο εκπαίδευσης $\mathcal{D}=\{(X_i, Y_i)\}_{i=1}^{N}$, όπου $X_i$ είναι \en{MRI} όγκος και $Y_i$
η αντίστοιχη δυαδική μάσκα πλακούντα. Θέλουμε να βρούμε παραμέτρους $\theta$ ώστε το μοντέλο $f_\theta$ να ελαχιστοποιεί
μία συνάρτηση απώλειας:
\begin{equation}
\theta^\star = \arg\min_{\theta} 
\sum_{i=1}^{N} \mathcal{L}\big(f_\theta(X_i), Y_i\big).
\end{equation}

Στην τμηματοποίηση, η επιλογή $\mathcal{L}$ είναι καθοριστική. Η απλή \en{binary cross-entropy} είναι χρήσιμη ως \en{per-voxel} ταξινόμηση, όμως συχνά 
συνδυάζεται με απώλειες επικάλυψης (π.χ. \en{Dice loss}), ώστε να μετριαστεί η ανισορροπία κλάσεων (το υπόβαθρο
καταλαμβάνει μεγάλο μέρος των \en{voxels}) \cite{sudre2017gdl}. Μία γνωστή \en{3D} αρχιτεκτονική που ανέδειξε τη χρησιμότητα 
απωλειών επικάλυψης είναι το \en{V-Net} \cite{milletari2016vnet}.

\section{Συνελικτικά Νευρωνικά Δίκτυα (\en{CNNs})}
\label{sec:cnns}

Τα \en{CNNs} αποτελούν την κυρίαρχη οικογένεια μοντέλων για εικόνες, καθώς επεξεργάζονται εισόδους \textit{δομημένες σε πλέγμα} και αξιοποιούν τη διαδικασία της \textbf{συνέλιξης} (\en{convolution}). 
Με κοινή χρήση βαρών σε όλο το πεδίο εισόδου, η ίδια ανίχνευση προτύπων εφαρμόζεται σε διαφορετικές χωρικές θέσεις,
δηλαδή παρουσιάζουν \textbf{ισοδυναμία ως προς μετατοπίσεις} (\en{translation equivariance}) \cite{aggarwal2018neural}.
Οι συνελίξεις λειτουργούν
ως φίλτρα που ανιχνεύουν τοπικά πρότυπα (άκρα, υφές, σχήματα), ενώ η ιεραρχική στοίβαξη πολλών στρωμάτων επιτρέπει την
αναπαράσταση ολοένα και πιο σύνθετων δομών.

Στην πράξη, τα \en{CNNs} χρησιμοποιούνται ευρέως σε προβλήματα όρασης, επειδή μαθαίνουν ιεραρχικά χαρακτηριστικά και κλιμακώνονται αποτελεσματικά με το μέγεθος των δεδομένων εκπαίδευσης \cite{krizhevsky2012imagenet}.
Στη συνέχεια, αρχιτεκτονικές με
\textbf{υπολειμματικές συνδέσεις} (\en{residual connections}) επέτρεψαν την εκπαίδευση πολύ βαθιών δικτύων
\cite{he2016resnet}.

\subsection{Η συνέλιξη σε \en{2D/3D} δεδομένα και η έννοια του \en{receptive field}}

Η βασική πράξη της συνέλιξης μπορεί να θεωρηθεί ως γραμμικός μετασχηματισμός με \textbf{κοινή χρήση βαρών} σε όλα τα χωρικά
σημεία. Για \en{3D} όγκους, μια \en{3D} συνέλιξη εφαρμόζει πυρήνα $K$ διαστάσεων $k_x\times k_y\times k_z$ και παράγει
χαρακτηριστικά που κωδικοποιούν τοπικές χωρικές σχέσεις μέσα στον όγκο. Η τοπικότητα αυτή είναι ιδιαίτερα χρήσιμη στην
τμηματοποίηση, όπου τα όρια ενός οργάνου καθορίζονται από \textit{τοπικές} μεταβολές έντασης και υφής.

Καθώς το βάθος του δικτύου αυξάνεται, το \textbf{\en{receptive field}} ενός νευρώνα μεγαλώνει (μέσω διαδοχικών συνελίξεων και
υποδειγματοληψιών), επιτρέποντας στο μοντέλο να ενσωματώνει ευρύτερο συμφραζόμενο (\en{wider context}). Παρ’ όλα αυτά, η αύξηση αυτή είναι
\textit{έμμεση} και εξαρτάται από το βάθος και τη σχεδίαση του δικτύου.

\subsection{Αρχιτεκτονικές πυκνής πρόβλεψης: \en{FCN}, \en{encoder--decoder} και \en{U-shaped} σχεδίαση}
Η τμηματοποίηση απαιτεί \textbf{πυκνή} πρόβλεψη (ετικέτα ανά \en{pixel/voxel}). Μια θεμελιώδης ιδέα για αυτόν τον σκοπό
είναι τα \en{Fully Convolutional Networks (FCNs)}, τα οποία αντικαθιστούν τα πλήρως συνδεδεμένα στρώματα με συνελικτικά,
ώστε το δίκτυο να παράγει χάρτες χαρακτηριστικών (\en{feature maps}) \cite{long2015fcn}.

Στην ιατρική τμηματοποίηση, η αρχιτεκτονική \en{U-Net} είναι ιδιαίτερα επιδραστική, καθώς συνδυάζει:
\begin{itemize}
  \item έναν \textbf{κωδικοποιητή} (\en{encoder}) που συμπιέζει την πληροφορία σε πολλαπλές κλίμακες (κωδικοποιόντας το «τι»),
  \item έναν \textbf{αποκωδικοποιητή} (\en{decoder}) που ανακατασκευάζει την ανάλυση (βρίσκοντας το «πού»),
  \item \textbf{συνδέσεις παράκαμψης} (\en{skip connections}) που μεταφέρουν λεπτομέρεια υψηλής ανάλυσης από τον κωδικοποιητή στον αποκωδικοποιητή, βελτιώνοντας την χωρική ακρίβεια \cite{ronneberger2015unet}.
\end{itemize}
Για ογκομετρικά δεδομένα, η λογική αυτή επεκτάθηκε σε \en{3D U-Net} με \en{3D} συνελίξεις, επιτρέποντας αξιοποίηση της
πληροφορίας κατά μήκος όλων των αξόνων του όγκου \cite{cicek20163dunet}.

\subsection{Κανονικοποίηση και σταθεροποίηση εκπαίδευσης}
Σε βαθιά δίκτυα, η σταθερότητα εκπαίδευσης επηρεάζεται από την κατανομή ενεργοποιήσεων. Η \en{Batch Normalization} είναι
κλασική τεχνική επιτάχυνσης και σταθεροποίησης \cite{ioffe2015batchnorm}, όμως στη \en{3D} τμηματοποίηση το \en{batch size}
είναι συχνά πολύ μικρό λόγω μνήμης. Σε τέτοιες περιπτώσεις, τεχνικές όπως η \en{Group Normalization} μπορεί να είναι
πρακτικά πιο σταθερές \cite{wu2018groupnorm}.

\subsection{Προσοχή και \en{CNNs}: \en{Attention U-Net}}
Η συγκεκριμένη ``προσοχή'' δεν ταυτίζεται απαραίτητα με τους \en{Transformers}. Στα \en{CNN}-βασισμένα μοντέλα, έχουν προταθεί
\textbf{\en{attention gates}} που «φιλτράρουν» τις \en{skip connections} ώστε να τονίζουν περιοχές σχετικές με το ενδιαφερόμενο όργανο. Ένα κλασικό παράδειγμα είναι το \en{Attention U-Net} \cite{oktay2018attentionunet}, το οποίο μπορεί να βελτιώσει
την εστίαση σε μικρές ή πολύπλοκες δομές χωρίς να απαιτεί πλήρη \en{self-attention} σε όλο τον όγκο.

\subsection{Πλεονεκτήματα και περιορισμοί των \en{CNNs}}
Τα κύρια πλεονεκτήματα για τμηματοποίηση είναι:
\begin{itemize}
  \item Υψηλή \textbf{αποδοτικότητα} (η συνέλιξη είναι υπολογιστικά ευνοϊκή).
  \item Έμφαση σε \textbf{τοπικά μορφολογικά στοιχεία}, χρήσιμα για ακριβή όρια.
  \item Ταχύτερα σε \textbf{μικρότερα σύνολα δεδομένων} σε σχέση με καθαρά \en{Transformer} μοντέ\-λα.
\end{itemize}

Κύριος περιορισμός είναι ότι η \textbf{παγκόσμια πληροφορία} (\en{global context}) δεν είναι άμεσα διαθέσιμη: οι μακρινές
χωρικές συσχετίσεις μοντελοποιούνται έμμεσα μέσω βάθους/υποδειγματοληψιών, και η αποτελεσματικότητα αυτής της διαδικασίας
εξαρτάται έντονα από τη σχεδίαση του δικτύου (βάθος, μέγεθος φίλτρου \en{(kernels)}, κ.\,λπ.).

\section{Μοντέλα \en{Attention} και \en{Transformers}}
\label{sec:transformers}

Οι \en{Transformers} εισήχθησαν ως αρχιτεκτονική ακολουθιών με μηχανισμό \textbf{αυτο-προσοχής} (\en{self-attention})
\cite{vaswani2017attention}. Η ιδέα είναι ότι κάθε στοιχείο της εισόδου μπορεί να «συσχετιστεί» \textbf{άμεσα} με άλλα στοιχεία,
επιτρέποντας μοντελοποίηση μακρινών εξαρτήσεων χωρίς να απαιτείται πολύ μεγάλο βάθος όπως στα προαναφερθέντα \en{CNNs}.
Το πλεονέκτημα αυτό συνοδεύεται απο τετραγωνικό κόστος ως προς το πλήθος \en{tokens}, κάτι που γίνεται κρίσιμο σε υψηλής ανάλυσης \en{2D/3D} εισόδους.

\subsection{Βασικός μηχανισμός \en{self-attention}}
Για μια ακολουθία $n$ \en{tokens} (π.\,χ. \en{patches} εικόνας), το \en{self-attention} υπολογίζει:
\begin{equation}
\mathrm{Att}(Q,K,V)=\mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V,
\end{equation}
όπου $Q,K,V$ (\en{queries, keys, values}) είναι γραμμικοί μετασχηματισμοί των \en{tokens}.
Η \en{multi-head} εκδοχή εφαρμόζει πολλαπλές τέτοιες προσοχές παράλληλα, ώστε το μοντέλο να μαθαίνει διαφορετικούς τύπους
συσχετίσεων.

\subsection{\en{Self-Attention} και Εικόνες: προκλήσεις και λύσεις}
Ο κλασικός \en{self-attention} έχει $O(n^2)$ υπολογιστική και χωρική πολυπλοκότητα ως προς το πλήθος \en{tokens} $n$.
Στην όραση, όμως, το $n$ προκύπτει από το πλήθος \en{patches}: για \en{2D} εικόνα
$n \approx (H\cdot W)/P^2$, ενώ για \en{3D} όγκο
$n \approx (H\cdot W\cdot D)/P^3$, όπου $P$ είναι η ακμή του \en{patch}.
Ακόμη και με \en{patching}, το $n$ παραμένει μεγάλο για πρακτική \en{3D} εκπαίδευση.

Ο \en{Vision Transformer (ViT)} \cite{dosovitskiy2021vit} διατύπωσε τη \en{2D} όραση ως ακολουθία \en{patch tokens} με \en{positional embeddings}. Ωστόσο, για ιατρικά δεδομένα (συχνά μικρότερα σύνολα και \en{3D} όγκοι), απαιτούνται πρόσθετες σχεδιαστικές επιλογές ώστε να είναι πρακτική η εκπαίδευση.

\subsection{Μείωση κόστους: ιεραρχία, τοπικά παράθυρα και προσεγγίσεις γραμμικής προσοχής}
Κύριες κατευθύνσεις για να καταστεί ο \en{attention} πρακτικός σε μεγάλες
εισόδους είναι:
\begin{itemize}
  \item \textbf{Ιεραρχική αναπαράσταση} με υποδειγματοληψίες, ώστε να
        μειώνεται σταδιακά το πλήθος \en{tokens}.
  \item \textbf{Τοπική προσοχή σε παράθυρα} (\en{window attention}):
        ο \en{Swin Transformer} υπολογίζει προσοχή σε τοπικά παράθυρα και
        τα μετατοπίζει (\en{shifted windows}) για ανταλλαγή πληροφορίας
        μεταξύ τους, επιτυγχάνοντας αποδοτική κλιμάκωση \cite{liu2021swin}.
  \item \textbf{Προσεγγίσεις υπο-τετραγωνικής προσοχής}: π.\,χ.\ χαμηλόβαθμη
        προβολή (\en{Linformer}) \cite{wang2020linformer} ή στοχαστικές
        προσεγγίσεις με \en{random features} (\en{Performer})
        \cite{choromanski2021performer}.
\end{itemize}
Στο πλαίσιο ιατρικής τμηματοποίησης, συχνά προτιμώνται οι δύο πρώτες
στρατηγικές (ιεραρχία/παράθυρα), επειδή διατηρούν καλύτερα τη χωρική δομή
και είναι πιο ``φιλικές'' σε \en{3D} δεδομένα.

\subsection{Υβριδικά \en{Transformer}--\en{U-shape} μοντέλα}
Υβριδικά μοντέλα όπως τα \en{UNETR} \cite{hatamizadeh2022unetr} και \en{SwinUNETR} \cite{hatamizadeh2022swinunetr}
χρησιμοποιούν \en{Transformer}-ενισχυμένους κωδικοποιητές για καλύτερη κατανόηση συμφραζόμενων, διατηρώντας ταυτό\-χρονα
\en{decoder} που ανακτά λεπτομέρεια υψηλής ανάλυσης, όπως στο \en{U-Net}. Έτσι επιχειρεί\-ται συνδυασμός:
\begin{itemize}
  \item \textbf{παγκόσμιας/μακρινής συσχέτισης} (ισχυρό σημείο των \en{Transformers})
  \item με \textbf{ακριβή χωρική ανακατασκευή} (ισχυρό σημείο της \en{U-shaped} αποκωδικοποίησης).
\end{itemize}

\section{Μοντέλα Χώρου Καταστάσεων (\en{State Space Models}) και \en{Mamba}}
\label{sec:ssm_mamba}

Τα Μοντέλα Χώρου Καταστάσεων (\en{State Space Models - SSMs}) αποτελούν μια 
κλασική κατηγορία δυναμικών συστημάτων που περιγράφουν την εξέλιξη μιας 
κρυφής κατάστασης $h_t \in \mathbb{R}^N$ μέσω μιας ακολουθίας εισόδων 
$x_t \in \mathbb{R}^M$. Σε διακριτή μορφή, ένα γραμμικό, χρονικά αναλλοίωτο 
(\en{Linear Time-Invariant - LTI}) \en{SSM} περιγράφεται από:
\begin{align}
  \label{eq:ssm_recurrent}
h_{t} &= \overline{A} h_{t-1} + \overline{B} x_{t}, \\ 
y_{t} &= C h_{t} + D x_{t},
\end{align}
όπου $\overline{A}, \overline{B}, C, D$ είναι οι παράμετροι του μοντέλου. 
Αυτή η αναδρομική μορφή επιτρέπει θεωρητικά ``άπειρης'' μνήμη, 
αλλά για μεγάλες ακολουθίες ο υπολογισμός είναι αυστηρά διαδοχικός και, κατά 
συνέπεια, αργός. Ωστόσο, αξιοποιώντας την 
\en{LTI} ιδιότητα, η εξέλιξη του συστήματος μπορεί 
να υπολογιστεί ισοδύναμα ως γραμμική συνέλιξη (\en{global convolution}), 
επιτρέποντας τον πλήρη παραλληλισμό \cite{gu2021efficiently}.

Παρότι τα \en{LTI} \en{SSMs} είναι υπολογιστικά αποδοτικά (π.χ. μέσω της ισοδυναμίας
με γραμμ\-ική συνέλιξη), στην απλή γραμμ\-ική τους μορφή συχνά υστερούν σε εκφραστικότητα σε σύγκριση με
\en{Transformers} \cite{gu2023mamba, gu2021efficiently}. Ένας βασικός λόγος είναι ότι οι παράμετροι
(π.χ. $\overline{A}$ και $\overline{B}$) δεν εξαρτώνται από την είσοδο, άρα απουσιάζει εγγενής 
\en{content-based selection} \cite{gu2023mamba}. Αυτό\- οδήγησε σε δομημένα \en{SSMs} (π.χ. \en{S4}) και σε επιλεκτικά \en{SSMs} όπως το \en{Mamba}, όπου κρίσιμες παράμετροι επηρεάζονται από την είσοδο.

\subsection{Δομημένα Μοντέλα Χώρου Καταστάσεων (\en{Structured SSMs}) και Μοντελοποίηση Μακρινών Εξαρτήσεων}
Για να καταστούν τα \en{SSMs} ανταγωνιστικά σε προβλήματα με πολύ μεγάλες ακολουθίες (π.χ. ήχος, \en{DNA}), εισήχθησαν τα \textbf{Δομημένα Μοντέλα Χώρου Καταστάσεων (\en{Structured SSMs})}. 
Μοντέλα όπως το \textbf{\en{S4}} επιβάλλουν συγκεκριμένη δομή στον πίνακα κατάστασης, επιτρέποντας αποδοτικό υπολογισμό και στις δύο μορφές μέσω ειδικών αλγορίθμων \cite{gu2021efficiently}. Το \en{S4} επέδειξε ισχυρή απόδοση σε εργασίες που απαιτούν μοντελοποίηση μακρινών εξαρτήσεων \cite{gu2021efficiently,goel2022its}.

\subsection{\en{Mamba}: Επιλεκτικά Μοντέλα Χώρου Καταστάσεων και Γραμμική Κλιμάκωση}

Παρόλο που τα δομημένα \en{SSMs} (όπως το \en{S4}) είναι αποτελεσματικά σε συνεχή σήματα,
αντι\-μετωπίζουν δυσκολίες σε διακριτά δεδομένα όπως το κείμενο, 
όπου η ικανότητα \textbf{επιλο\-γής με βάση το περιεχόμενο} (\en{content-based selection}) είναι κρίσιμη \cite{gu2023mamba}. 
Το μοντέλο \en{\textbf{Mamba}} επιλύει αυτόν τον περιορισμό εισάγοντας εκλεκτικούς χώρους καταστάσεων (\en{selective state spaces}): 
κρίσιμες παράμετροι (π.χ. $\overline{B}, C$ και το βήμα διακριτοποίησης $\Delta$) γίνονται συναρτήσεις της τρέχουσας εισόδου $x_t$, επιτρέποντας στο μοντέλο να ``αποφασίζει'' δυναμικά ποια πληροφορία θα διατηρήσει.

Η αλλαγή αυτή δυσκολεύει τη χρήση καθαρής συνελικτικής μορφής, οπότε το \en{Mamba} βασίζεται σε \textbf{υλοποίηση βελτιστοποιημένη για \en{GPU}} (\en{hardware-aware scan}), 
ώστε να διατηρεί \textbf{γραμμική πολυπλοκότητα} $Ο(L)$ ως προς το μήκος $L$ της ακολουθίας, με υψηλή πρακτική αποδοτικότητα \cite{gu2023mamba}.

\subsection{Εφαρμογή \en{SSMs/Mamba} σε \en{3D} Τμηματοποίηση}
Για την εφαρμογή των \en{SSMs} σε ογκομετρικά δεδομένα, απαιτείται η 
σειριοποίηση του όγκου σε ακολουθία (\en{serialization}). Μια τυπική 
μεθοδολογία περιλαμβάνει:
\begin{enumerate}
  \item Διαχωρισμό του \en{3D} όγκου σε \en{patches} και 
  προβολή σε \en{tokens}.
  \item Ιεραρχική αρχιτεκτονική τύπου \en{U-shaped} για \en{multi-scaling processing}.
  \item Ενσωμάτωση \en{Mamba blocks} που αντικαθιστούν τα στρώματα προσοχής ή μέρος των συνελικτικών \en{blocks}.
\end{enumerate}
Με αυτόν τον τρόπο, το μοντέλο μπορεί να συλλάβει \textbf{μακρινές χωρικές 
εξαρτήσεις} σε ολό\-κληρο τον όγκο με σημαντικά χαμηλότερο υπολογιστικό κόστος σε σχέση με πλήρη
\en{self-attention}. Πρόσφατες προσεγγίσεις όπως το \en{SegMamba} \cite{xing2024segmamba} δείχνουν 
ότι η ιδέα αυτή μπορεί να δώσει ανταγωνιστική ακρίβεια σε \en{3D} τμηματοποίηση.

\subsection{Συγκριτική Επισκόπηση}
Συνοψίζοντας τη θεωρητική προοπτική για την τμηματοποίηση πλακούντα:
\begin{itemize}
  \item \textbf{\en{CNNs / U-Net}}: Αποδοτικά για τοπικό σήμα, αλλά η ``καθολική'' πληροφορία εισάγεται έμμεσα (μέσω βάθους).
  \item \textbf{\en{Transformers}}: Παρέχουν άμεση καθολική συσχέτιση 
  (\en{global context}), αλλά το τετρα\-γωνικό κόστος $O(L^2)$ είναι συχνά 
  απαγορευτικό για υψηλής ανάλυσης \en{3D} δεδομένα.
  \item \textbf{\en{SSMs / Mamba}}: Στοχεύουν σε \textbf{γραμμική κλιμάκωση} $O(n)$ 
  με ικανότητα μοντελοποί\-ησης μακρινών εξαρτήσεων, αποτελώντας υποσχόμενη μέση λύση.
\end{itemize}


\section{Βελτιστοποίηση και πρακτικές εκπαίδευσης}
\label{sec:training_practices}

Η εκπαίδευση βαθιών δικτύων πραγματοποιείται συνήθως με στοχαστική κλίση
κατά\-βασης και παραλλαγές της. Στο πλαίσιο αυτό, ο \en{AdamW} αποτελεί
διαδεδομένη επιλογή λόγω του αποσυνδεδεμένου (\en{decoupled})
\en{weight decay} \cite{loshchilov2019adamw}.

Στον \en{Adam}, οι εκθετικοί κινητοί μέσοι πρώτης και δεύτερης ροπής
υπολογίζονται ως:
\[
m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t, \qquad
v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2.
\]
Όταν η κανονικοποίηση εφαρμόζεται ως \en{$L^2$-norm} μέσα στο \en{gradient}, ισχύει:
\[
g_t = \nabla_{\theta}\mathcal{L}(\theta_t) + \lambda \theta_t.
\]
Σε αυτή την περίπτωση, ο όρος κανονικοποίησης αλληλεπιδρά με την
προσαρμοστική κλίμακα του \en{Adam}. Αντίθετα, στον \en{AdamW} η συρρίκνωση
των βαρών εφαρμόζεται ξεχωριστά από τον προσαρμοστικό όρο ενημέρωσης:
\[
\theta_{t+1} = \theta_t - \eta \frac{\hat m_t}{\sqrt{\hat v_t} + \epsilon}
- \eta \lambda \theta_t.
\]
Η αποσύνδεση αυτή καθιστά τη ρύθμιση του \en{weight decay} πιο προβλέψιμη ως
υπερπαράμετρο και συχνά συνδέεται με καλύτερη γενίκευση σε πρακτικές εφαρμογές
\cite{loshchilov2019adamw}.

Στην \en{3D} τμηματοποίηση ακολουθούνται κατά κανόνα οι εξής πρακτικές:
\begin{itemize}
  \item \textbf{Μικρό \en{batch size}} λόγω περιορισμών στη μνήμη της \en{GPU}.
  \item \textbf{Εκπαίδευση με \en{patches}} και ισορροπία θετικών/αρνητικών δειγμάτων.
  \item \textbf{Μεταβαλλόμενος ρυθμός μάθησης} (\en{learning rate schedules}) για επίτευξη βέλτιστης σύγκλισης (π.χ. \en{CyclicLR, Cosine Annealing} \cite{lr_schedulers}).
  \item \textbf{Κανονικοποίηση} (π.χ. \en{GroupNorm} ή \en{InstanceNorm}) όταν το \en{batch size} είναι πολύ μικρό.
\end{itemize}

Στην παρούσα εργασία, οι παραπάνω επιλογές εφαρμόζονται ενιαία στα πειράματα, με στόχο τη διασφάλιση μιας \textbf{δίκαιης σύγκρισης} μεταξύ διαφορετικών αρχιτεκτονικών. Με τον τρόπο αυτό, απομονώνεται η επίδραση της σχεδίασης του μοντέλου στην ακρίβεια τμηματοποίησης πλακούντα σε εικόνες \en{MRI}, ελαχιστοποιώντας την επιρροή των υπερπαραμέτρων εκπαίδευσης \cite{cardoso2022monai}.
