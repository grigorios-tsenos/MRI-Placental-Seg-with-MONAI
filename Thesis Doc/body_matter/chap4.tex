% Chapter 4
% ΟΙ ΑΡΧΙΤΕΚΤΟΝΙΚΕΣ ΠΟΥ ΧΡΗΣΙΜΟΠΟΙΩ ΣΤΑ ΠΕΙΡΑΜΑΤΑ ΚΑΙ Η ΛΕΙΤΟΥΡΓΙΑ ΤΟΥΣ 
\chapter{Αρχιτεκτονικές Τμηματοποίησης που Χρησιμοποιήθηκαν}
\label{ch:architectures}

\InitialCharacter{Σ}το κεφάλαιο αυτό παρουσιάζονται οι αρχιτεκτονικές που χρησιμοποιήθηκαν στα πειράματα της παρούσας εργασίας. 
Η ανάλυση εστιάζει στον τρόπο με τον οποίο κάθε μοντέλο συλλέγει \en{context} (τοπικό ή μακρινής εμβέλειας), καθώς και στους συμβιβασμούς ακρίβειας, μνήμης και υπολογιστικού κόστους στην 
\en{3D} ιατρική τμηματοποίηση.


\section{Το πλαίσιο \en{MONAI}}
\label{sec:monai_framework}

Το \en{MONAI} (\en{Medical Open Network for AI}) είναι ένα ανοικτού κώδικα,
\en{PyTorch}-based πλαίσιο που στοχεύει στη διευκόλυνση ανάπτυξης και
αξιολόγησης μοντέλων Βαθιάς Μάθησης για εφαρμογές υγείας, με έμφαση στην
ιατρική απεικόνιση \cite{cardoso2022monai}. Παρέχει:
\begin{itemize}
  \item \textbf{Μετασχηματισμούς} (\en{transforms}) για \en{3D} δεδομένα
        (φόρτωση, \en{resampling}, κανονικοποίηση, \en{augmentation}).
  \item \textbf{Έτοιμες αρχιτεκτονικές} τμηματοποίησης και
        \en{building blocks} (συνελικτικά/υπολειμματικά blocks,
        κανονικοποιήσεις, \en{upsampling}).
  \item \textbf{Απώλειες και μετρικές} (π.\,χ.\ \en{Dice-based losses} και
        \en{Dice/IoU} μετρικές) και τυποποιημένες ροές εκπαίδευσης.
  \item \textbf{Μηχανισμούς \en{inference}} όπως \en{sliding-window inference}
        για μεγάλους \en{3D} όγκους.
\end{itemize}
Η χρήση του \en{MONAI} επιτρέπει πιο \textbf{αναπαραγώγιμο} και
\textbf{συγκρίσιμο} πειραματικό πρωτόκολλο, καθώς πολλές επιλογές
υλοποίησης είναι κοινές και ελεγχόμενες \cite{cardoso2022monai}. Για
περαιτέρω τεχνικές λεπτομέρειες παραπέμπουμε στην επίσημη τεκμηρίωση και
στο αποθετήριο του έργου \cite{monai_docs, monai_github}.


\subsection{Υλοποίηση μοντέλων στην παρούσα εργασία}
\label{subsec:implementation_sources}

Στην παρούσα εργασία, όλες οι αρχιτεκτονικές \en{UNet}, \en{DynUNet}, \en{SegResNet}, 
\en{AttentionUnet}, \en{UNETR} και \en{SwinUNETR} χρησιμοποιούνται μέσω των αντίστοιχων υλοποιήσεων 
του \en{MONAI} \cite{cardoso2022monai}. Εξαίρεση αποτελεί το \en{SegMamba}, το οποίο \textbf{δεν διατίθεται στο \en{MONAI}} και ενσωματώνεται μέσω της \textbf{επίσημης υλοποίησης των συγγραφέων} 
\cite{xing2024segmamba,segmamba_github}. Για να διατηρηθεί \textbf{δίκαιη σύγκριση}, το \en{SegMamba} εκπαιδεύεται και αξιολογείται με το ίδιο πρωτόκολλο προεπεξεργασίας, 
δειγματοληψίας \en{patches}, απώλειας, 
μετρικών και \en{sliding-window inference} που εφαρμόζεται και στα \en{MONAI-}μοντέλα.

\section{Οικογένειες μοντέλων που συγκρίνονται}
\label{sec:model_families}

Η αρχιτεκτονικές ομαδοποιούνται ως εξής:
\begin{itemize}
  \item \textbf{\en{CNN-based}}: 
    \en{UNet}, \en{DynUNet}, \en{SegResNet}, \en{AttentionUnet}.
  \item \textbf{\en{Transformer}-ενισχυμένα}: 
    \en{UNETR}, \en{SwinUNETR}.
  \item \textbf{\en{SSM/Mamba-based}}: 
    \en{SegMamba} (εκτός \en{MONAI}, μέσω επίσημου κώδικα).
\end{itemize}

\section{\en{UNet} και \en{3D U-Net}}
\label{sec:unet}

Το \en{U-Net} \cite{ronneberger2015unet} αποτελεί σημείο αναφοράς στην ιατρική τμηματοποίηση. Πρόκειται για συμμετρική αρχιτεκονική \ref{figure4.1} \en{encoder-decoder} με \en{skip connections}:
\begin{itemize}
  \item Ο \textbf{κωδικοποιητής} (\en{encoder}) μειώνει την ανάλυση (\en{downsampling}), 
        αυξάνοντας τα κανάλια, ώστε να κωδικοποιεί υψηλού επιπέδου χαρακτηριστικά.
  \item Ο \textbf{αποκωδικοποιητής} (\en{decoder}) ανακτά την ανάλυση (\en{upsampling}) και συνδυάζει πληροφορία υψηλής ανάλυσης απο τον \en{encoder}, ώστε να βελτιώνει την ακρίβεια εντοπισμού ορίων. 
\end{itemize}

\begin{figure}[!ht] \centering
\includegraphics[width=0.7\linewidth]{figures/u-net-architecture.png}
 \caption{Δομή αρχιτεκτονικής \en{U-net}}
\label{figure4.1}
\end{figure}

Για ογκομετρικά δεδομένα, η λογική επεκτείνεται σε \en{3D U-Net} με \en{3D} συνελίξεις \cite{cicek20163dunet}. 
Τα \en{skip connections} είναι κρίσιμα για ακριβή \en{localization}, καθώς μετριάζουν την απώλεια 
λεπτομέρειας στα στάδια υποδειγματοληψίας.

\paragraph{Σχόλιο.}
Το \en{UNet} τείνει να αποδίδει πολύ καλά σε τοπικά μορφολογικά μοτίβα και οριακές μεταβολές. Ο κύριος περιορισμός είναι ότι το \en{global context} εισάγεται έμμεσα μέσω βάθους και \en{multi-scaling} συμπίεσης.

\section{\en{DynUNet}}
\label{sec:dynunet}

Το \en{DynUNet} (στη \en{MONAI} υλοποίηση) είναι παραλλαγή \en{3D UNet} που επιτρέπει \textbf{δυναμική} προσαρμογή της τοπολογίας (στάδια, \en{strides}, μεγέθη μυρήνων) με βάση τις διαστάσεις/ανά\-λυση εισόδου 
και το \en{patch size}.
Η φιλοσοφία συνδέεται με το \en{nnU-Net}, όπου έδειξε ότι συστηματικές επιλογές προεπεξεργασίας και ``εύλογες'' αρχιτεκτονικές αποφάσεις μπορούν να δώσουν ισχυρά αποτελέσματα σε πολλά \en{datasets}
\cite{isensee2021nnunet}.

\paragraph{Κεντρική ιδέα.}
Η προσαρμογή επιδιώκει επαρκές \en{receptive field} με ελεγχόμενο κόστος, 
ώστε να μην χάνεται υπερβολικά χωρική πληροφορία σε \en{3D} δεδομένα. 
Συχνά υποστηρίζεται και \en{deep supervision}, που μπορεί να βοηθήσει στη 
σύγκλιση σε ρυθμίσεις μικρού \en{batch size} \cite{isensee2021nnunet}.

\section{\en{SegResNet}: υπολειμματικό \en{encoder-decoder}}
\label{sec:segresnet}

Το \en{SegResNet} χρησιμοποιεί υπολειμματικά \en{blocks} τύπου \en{ResNet} 
\cite{he2016resnet} μέσα σε \en{encoder-decoder} σχήμα. Μία επιδραστική 
μορφή αυτού του στυλ για \en{3D} \en{MRI} τμηματοποίηση εί\-ναι η προ\-σέ\-γγιση 
του \en{Myronenko} \cite{myronenko2018segresnet}, όπου η υπολειμματική δομή συνδυάζεται 
και με \en{\-autoencoder regularization}. 


\begin{figure}[!ht] \centering
\includegraphics[width=0.4\linewidth]{./figures/Res connection.png}
 \caption{Μηχανισμός \en{residual connection} στην αρχιτεκτονική \en{ResNet} \cite{he2016resnet}}
\label{figure4.2}
\end{figure}

\paragraph{Γιατί υπολείμματα.}
Οι υπολειμματικές συνδέσεις (\ref{figure4.2}) διευκολύνουν τη ροή κλίσεων και επιτρέπουν 
εκπαίδευση βαθύτερων δικτύων με σταθερότερη σύγκλιση \cite{he2016resnet}.
Αυτό είναι χρήσιμο όταν απαιτείται μεγαλύτερο βάθος για ενσωμάτωση 
συμφραζομένων σε \en{3D} όγκους.

\section{\en{AttentionUnet}: προσοχή στις \en{skip connections}}
\label{sec:attentionunet}

Η \en{AttentionUNet} εισάγει \en{attention gates} \ref{figure4.3} στις \en{skip connections}, ώστε το δίκτυο 
να ενισχύει περιοχές σχετικές με το \en{ROI} και να καταστέλλει άσχετο υπόβαθρο \cite{oktay2018attentionunet}. Το \en{gate} συνδυάζει πληροφορία απο ρηχότερα επίπεδα (λεπτομέρεια) και βαθύτερα επίπεδα (συμφραζόμενο), 
υπολογίζοντας βάρη εστίασης.

\begin{figure}[!ht] \centering
\includegraphics[width=0.6\linewidth]{./figures/AttUNET attention gate.png}
 \caption{Προτεινόμενος σχεδιασμός του \en{attention gate}}
\label{figure4.3}
\end{figure}

\paragraph{Πότε βοηθά.} 
Η προσοχή είναι ιδιαίτερα χρήσιμη όταν το \en{ROI} είναι μικρό, υπάρχει ισχυρή ανισορροπία κλάσεων ή όταν δομές έχουν παρόμοια ένταση με το υπόβαθρο.

\section{\en{UNETR}: \en{ViT encoder} με \en{U-shaped decoder}}
\label{sec:unetr}

Το \en{UNETR} \cite{hatamizadeh2022unetr} αποτελεί υβριδικό σχήμα, όπου ο \en{encoder} βασίζεται σε \en{Transformer}
(\en{ViT-like}) και ο \en{decoder} διατηρεί τη λογική ανακατασκευής τύπου \en{U-Net}. Η είσοδος χωρίζεται σε μη επικαλυπτόμενα 
\textbf{\en{3D patches}} που προβάλλονται σε \en{tokens}, ενώ το \en{self-attention} επιτρέπει άμεση μοντελοποίηση μακρινών συσχετίσεων.

\paragraph{Σχόλιο κόστους.}
Το πλεονέκτημα του \en{global context} συνοδεύεται απο αυξημένο κόστος μνήμης/χρόνου, το οποίο εξαρτάται έντονα από το \en{patch grid}. Για αυτό, η επιλογή \en{patch size} και ο τρόπος inference (π.χ. \en{sliding-window}) είναι κρίσιμα πρακτικά σημεία \cite{hatamizadeh2022unetr}.

\section{\en{SwinUNETR}: ιεραρχικός \en{Swin encoder}}
\label{sec:swinunetr}
Το \en{SwinUNETR} βασίζεται σε \en{Swin Transformer encoder} με ιεραρχία και \en{window-based attention} 
\cite{hatamizadeh2022swinunetr,liu2021swin}. 

Αντί για πλήρη \en{attention} σε όλα τα \en{tokens}, η προσοχή υπολογίζεται μέσα σε τοπικά παράθυρα. 
Τα παράθυρα μετατοπίζονται (\en{shifted windows}) μεταξύ διαδοχικών \en{blocks}, επιτρέποντας ανταλλαγή πληροφορίας μεταξύ γειτονικών περιοχών.

\paragraph{Ιεραρχία έναντι \en{ViT}.}
Σε ένα κλασικό \en{ViT} ο αριθμός και η διάταξη των \en{tokens} παραμένουν σταθερά σε όλο τον \en{encoder} (μονο-κλίμακη αναπαράσταση). 
Ο \en{Swin} χωρίζεται σε διακριτά \en{stages} και εφαρμόζει \en{patch merging}, 
μειώνοντας προοδευτικά τη χωρική ανάλυση και δημιουργώντας πυραμίδα χαρακτηριστικών πολλαπλών κλιμάκων (\ref{figure4.4}).
\begin{figure}[!ht] \centering
\includegraphics[width=0.4\linewidth]{./figures/swin vs vit.png}
 \caption{Αριστερά: \en{Swin} με ιεραρχικά \en{stages} (πολλαπλές κλίμακες μέσω \en{patch merging}).\\
 Δεξιά: \en{ViT} χωρίς ιεραρχία, με σταθερή ανάλυση \en{tokens} σε όλο τον \en{encoder}.}
\label{figure4.4}
\end{figure}
\paragraph{Γιατί ταιριάζει σε \en{3D}.}
Η τοπική προσοχή μειώνει το υπολογιστικό κόστος σε μεγάλους όγκους, ενώ η ιεραρχία (μέσω \en{patch merging}) μειώνει προοδευτικά το πλήθος \en{tokens}. 
Τα ενδιάμεσα επίπεδα πολλαπλών κλιμάκων τροφοδοτούν έναν \en{U-shaped decoder} με \en{skip connections}, διατηρώντας λεπτομέρεια και χωρική ακρίβεια.

\section{\en{SegMamba}: \en{Mamba/SSM blocks} σε \en{U-shape} σχεδίαση}
\label{sec:segmamba}

Το \en{SegMamba} προτείνει μια \en{3D} αρχιτεκτονική τμηματοποίησης που
ενσωματώνει \en{Mamba}-βασισμένα \en{SSM} blocks για μοντελοποίηση
μακρινών εξαρτήσεων με ευνοϊκή κλιμάκωση σε σχέση με πλήρες
\en{self-attention} \cite{xing2024segmamba}. Ο μηχανισμός \en{Mamba}
εισάγει \en{selective state spaces}, επιτρέποντας \en{content-dependent}
παραμετροποίηση που λειτουργεί ως εναλλακτική της \en{attention} σε
μεγάλες ακολουθίες \cite{gu2023mamba}.

\paragraph{Σημείωση υλοποίησης.}
Σε αντίθεση με τα υπόλοιπα μοντέλα που προέρχονται από \en{MONAI},
η αρχιτεκτονική \en{SegMamba} υλοποιείται στην παρούσα εργασία μέσω του
\textbf{επίσημου αποθετηρίου} των συγγραφέων \cite{segmamba_github}. Η
ενσωμάτωση γίνεται με τρόπο που να επιτρέπει κοινό πειραματικό πρωτόκολλο
(προεπεξεργασία, \en{patch sampling, loss, metrics} και \en{inference}), ώστε η
σύγκριση να παραμένει συνεπής.

\paragraph{Αναμενόμενη συμπεριφορά.}
Η σχεδίαση στοχεύει να συνδυάσει:
\begin{itemize}
  \item συλλογή \textbf{μακρινής} εμβέλειας πληροφοριών σε πολλαπλές κλίμακες, 
  \item \en{U-shaped} αποκωδικοποίηση για διατήρηση χωρικής ακρίβειας, 
  \item βελτιωμένη διαχείριση πόρων σε \en{3D} δεδομένα, σε σύγκριση με \en{Transformer encoders} αντίστοιχης ανάλυσης.
\end{itemize}
  
\section{Συζήτηση: αναμενόμενοι συμβιβασμοί}
\label{sec:tradeoffs}

Συνοψίζοντας, οι οικογένειες μοντέλων διαφέρουν ως προς το \textbf{πώς} συλλέγουν συμφραζό\-μενο:
\begin{itemize}
  \item \textbf{\en{CNN-based}}: ισχυρά σε όρια/υφές και αποδοτικά, αλλά το \en{global context} εισάγεται έμμεσα.
  \item \textbf{\en{Transformer-based}}: ισχυρό \en{global context}, αλλά αυξημένες απαιτήσεις μνήμης/χρόνου.
  \item \textbf{\en{SSM/Mamba-based}}: στόχος η μακρινής εμβέλειας μοντελοποίηση με καλύτερη κλιμά\-κωση, κάτι ιδιαίτερα ελκυστικό σε \en{3D} τμηματοποίηση.
\end{itemize}

Στο πειραματικό μέρος παρουσιάζεται το κοινό πρωτόκολλο εκπαίδευσης/αξιολόγησης που επιτρέπει να αποτιμηθούν εμπειρικά 
οι παραπάνω συμβιβασμοί για την τμηματοποίηση πλακούντα σε \en{MRI}. 
